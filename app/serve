#! /usr/bin/env python

# major part of code sourced from aws sagemaker example:
# https://github.com/aws/amazon-sagemaker-examples/blob/main/advanced_functionality/scikit_bring_your_own/container/decision_trees/predictor.py

import io
import numpy as np, pandas as pd
import json
from fastapi import FastAPI
from fastapi.responses import JSONResponse, FileResponse
from fastapi import Depends, FastAPI, File, UploadFile, status, HTTPException
from typing import Union
import traceback
import sys
import os, warnings
from tempfile import NamedTemporaryFile
import uvicorn

os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"  # or any {'0', '1', '2'}
warnings.filterwarnings("ignore")

import algorithm.utils as utils
from algorithm.model_server import ModelServer
from algorithm.model import clustering as model


prefix = "/opt/ml_vol/"
data_schema_path = os.path.join(prefix, "inputs", "data_config")
model_path = os.path.join(prefix, "model", "artifacts")
failure_path = os.path.join(prefix, "outputs", "errors", "serve_failure.txt")


# get data schema - its needed to set the prediction field name
# and to filter df to only return the id and pred columns
data_schema = utils.get_data_schema(data_schema_path)


# initialize your model here before the app can handle requests
model_server = ModelServer(model_path=model_path, data_schema=data_schema)


# The flask app for serving predictions
app = FastAPI()


async def gen_temp_file(ext: str = ".csv"):
    """Generate a temporary file with a given extension"""
    with NamedTemporaryFile(suffix=ext, delete=True) as temp_file:
        yield temp_file.name


@app.get("/ping", tags=["ping", "healthcheck"])
async def ping() -> dict:
    """Determine if the container is working and healthy."""
    response = f"Hello - I am {model.MODEL_NAME} model and I am at your service!"
    return {
        "success": True,
        "message": response,
    }


@app.post("/infer", tags=["inference", "json"], response_class=JSONResponse)
async def infer(input_: dict) -> dict:
    """Generate inferences on a single batch of data sent as JSON object.
    In this sample server, we take data as JSON, convert
    it to a pandas data frame for internal use and then convert the predictions back to JSON .
    """
    # Do the prediction
    try:  # Do the prediction
        data = pd.DataFrame.from_records(input_["instances"])
        print(f"Invoked with {data.shape[0]} records")
        inference_df = model_server.predict(data)
        # convert to the json response specification
        id_field_name = model_server.id_field_name
        predictions = []
        for rec in inference_df.to_dict(orient="records"):
            infer_obj = {}
            infer_obj[id_field_name] = rec[id_field_name]
            infer_obj["prediction"] = rec["prediction"]
            predictions.append(infer_obj)
        return {
            "success": True,
            "predictions": predictions,
        }
    except Exception as err:
        # Write out an error file. This will be returned as the failureReason to the client.
        trc = traceback.format_exc()
        with open(failure_path, "w") as s:
            s.write("Exception during inference: " + str(err) + "\n" + trc)
        # Printing this causes the exception to be in the training job logs, as well.
        print("Exception during inference: " + str(err) + "\n" + trc, file=sys.stderr)
        # A non-zero exit code causes the training job to be marked as Failed.
        return {
            "success": False,
            "message": f"Exception during inference: {str(err)} (check serve_failure.txt file for more details)",
        }


@app.post("/infer_file", tags=["inference", "csv"], response_class=JSONResponse)
async def infer_file(
    input_: UploadFile = File(...), temp=Depends(gen_temp_file)
) -> dict:
    """
    Do an inference on a single batch of data provided as a multi-part CSV file.
    In this sample server, we take data as CSV, convert it to a pandas dataframe
    for internal use and then convert the predictions to JSON.
    """
    data = None
    print(input_)

    # Convert from CSV to pandas
    if input_.content_type == "text/csv":
        data = await input_.read()
        temp_io = io.StringIO(data.decode("utf-8"))
        data = pd.read_csv(temp_io)
    else:
        return {
            "success": False,
            "message": f"Content type {input_.content_type} not supported (only CSV data allowed)",
        }
    # Do the prediction
    try:
        print(f"Invoked with {data.shape[0]} records")
        inference_df = model_server.predict(data)
        # convert to the json response specification
        id_field_name = model_server.id_field_name
        predictions = []
        for rec in inference_df.to_dict(orient="records"):
            infer_obj = {}
            infer_obj[id_field_name] = rec[id_field_name]
            infer_obj["prediction"] = rec["prediction"]
            predictions.append(infer_obj)
        return {
            "success": True,
            "predictions": predictions,
        }
    except Exception as err:
        # Write out an error file. This will be returned as the failureReason to the client.
        trc = traceback.format_exc()
        with open(failure_path, "w") as s:
            s.write("Exception during inference: " + str(err) + "\n" + trc)
        # Printing this causes the exception to be in the training job logs, as well.
        print("Exception during inference: " + str(err) + "\n" + trc, file=sys.stderr)
        # A non-zero exit code causes the training job to be marked as Failed.
        return {
            "success": False,
            "message": f"Exception during inference: {str(err)} (check failure file for more details)",
        }


if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8080)
